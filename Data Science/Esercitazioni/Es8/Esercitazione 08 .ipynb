{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Istruzioni\n",
    "Valgono le stesse istruzioni delle esercitazioni precedenti. \n",
    "\n",
    "Scrivi il programma Python nella cella sotto il testo dell'esercizio (o creane una nuova). Stampa sempre a video il risultato finale per verificare la correttezza dell'esercizio.  Talvolta richiamiamo alcuni concetti importanti in una cella di codice sotto il testo dell'esercizio, prova a eseguirla ed eventualmente modificarla per assicurarti di aver capito il necessario.\n",
    "\n",
    "**Nota.** Alcuni esercizi potrebbero richiedere una semplice risposta a delle domande. In questo caso potete scrivere la soluzione in una cella di tipo \"Markdown\".  \n",
    "\n",
    "\n",
    "## Consegna\n",
    "Valgono le regole delle esercitazioni precedenti.\n",
    "\n",
    "E' obbligatorio **consegnare la soluzione di tutti gli esercizi** (tranne quelli marcati come opzionali) **entro l'inizio della lezione successiva** (in questo caso entro Lunedi' prossimo), nell'apposito assignment su iCorsi.  Per consegnare:\n",
    "- eseguire l'intero notebook partendo da zero (`Kernel -> Restart & Run All`), e controllare che le soluzioni siano quelle attese;\n",
    "- esportare il notebook in formato html (`File -> Download as...`) e consegnare il file risultante.\n",
    "\n",
    "Nel caso non abbiate potuto completare uno o piu' esercizi, descrivete il problema incontrato e **consegnate comunque il file con il resto delle soluzioni**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 1 \n",
    "\n",
    "Si consideri il dataset visto a lezione e nell'esercitazione precedente sulla classificazione delle cellule tumorali.\n",
    "\n",
    "Quando si lavora con dati provenienti da esami clinici, in genere si dice che un'istanza è di classe positiva (+1) se il test clinico è positivo (es., presenza di tumore maligno), mentre l'istanza è di classe negativa (-1) se il test è negativo (es., il tumore NON è maligno). \n",
    "\n",
    "Il dataset utilizzato non usa questa convenzione. Eseguite la cella sotto per caricare il dataset e convertire la variabile target secondo la convezione descritta sopra (y = 1: tumore maligno; y = -1: tumore benigno). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "ds = sklearn.datasets.load_breast_cancer()\n",
    "\n",
    "x = ds.data\n",
    "yT = ds.target # Il datset originale usa la convezione yT = 0 per tumore maligno, yT = 1 per tumore benigno \n",
    "y = yT\n",
    "\n",
    "# Convertiamo la label secondo la convenzione: y = 1 per tumore maligno, y = -1 per tumore benigno\n",
    "y[yT == 1] = -1\n",
    "y[yT == 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "Dopo aver diviso in **maniera casuale** il dataset completo in set di training (70% delle osservazioni) e set di test (rimanente 30% percento), normalizzate ogni attributo come visto a lezione, in modo che le osservazioni di ogni attributo del training set abbiano media nulla e deviazione standard unitaria. Utilizzando media e deviazione standard **utilizzate per normalizzare gli attributi del training set**, riscalate anche le osservazioni del test set.\n",
    "\n",
    "- Calcolate media e deviazione standard di ogni attributo per le osservazioni del training set. Verificate che siano uguali a 0 e a 1, rispettivamente.\n",
    "\n",
    "- Calcolate media e deviazione standard di ogni attributo per le osservazioni del test set. Sono rispettivamente uguali a 0 e a 1? Perchè? \n",
    "\n",
    "Da ora in avanti, si considerino sempre gli attributi normalizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig set: \n",
      " Media: 62.10876596785595, \n",
      " Std: 229.3446999711776\n",
      "\n",
      "Normalized training set: \n",
      " Media: [-1.20869255e-15 -3.93292322e-15 -3.50361839e-15  2.23160407e-16\n",
      " -2.97863353e-15  9.79674187e-16  5.20521649e-16  2.25949912e-16\n",
      " -1.35005072e-15  3.02103401e-16  5.08247827e-16 -3.65425166e-16\n",
      " -1.17996065e-16 -6.12575317e-16 -1.19056077e-15 -2.49270175e-15\n",
      " -3.00429698e-16  1.11663889e-15 -3.51756591e-16 -1.23017174e-16\n",
      "  6.05880505e-16  3.82162197e-16 -1.60396543e-15 -1.79644128e-15\n",
      " -2.81461063e-16 -1.61233394e-16 -4.34604893e-16  7.29455580e-16\n",
      "  1.52753299e-15 -3.03609734e-15], \n",
      " Std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Normalized test set: \n",
      " Media: [-2.22986022e-15  1.46471529e-15 -6.09648784e-16 -1.86011051e-16\n",
      "  9.03760497e-16  6.94700957e-17 -2.75283370e-16 -1.03068950e-16\n",
      "  4.95380215e-16  4.56733417e-15  4.00589243e-16 -4.49283236e-16\n",
      "  2.91190074e-16  2.66843078e-16 -8.79088874e-16 -3.19432590e-16\n",
      " -3.27223628e-16 -2.61973679e-16 -2.59701292e-16  9.93357443e-16\n",
      " -6.31074140e-16  6.90805438e-16  6.70840901e-16  2.02242381e-16\n",
      " -1.54944284e-15  1.23358114e-16  5.99909985e-16 -4.17469827e-16\n",
      "  5.08365280e-16  1.72181957e-15], \n",
      " Std: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution (to be completed)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.3, random_state=42) # 30% nei test e il numero di volte che deve essere mescolato\n",
    "\n",
    "m,n = np.shape(xTrain) # Compute number of observations in training set and maximum number of features\n",
    "\n",
    "# Compute mean and std in training observations\n",
    "xMean = np.mean(xTrain)\n",
    "xStd = np.std(xTrain)\n",
    "\n",
    "print(f\"Trainig set: \\n Media: {xMean}, \\n Std: {xStd}\\n\")\n",
    "\n",
    "# Scale training data\n",
    "xTrainScaled = preprocessing.scale(xTrain)\n",
    "print(f\"Normalized training set: \\n Media: {np.mean(xTrainScaled, axis = 0)}, \\n Std: {np.std(xTrainScaled, axis = 0)}\\n\")\n",
    "\n",
    "# Scale test data\n",
    "xTestScaled = preprocessing.scale(xTest)\n",
    "print(f\"Normalized test set: \\n Media: {np.mean(xTestScaled, axis = 0)}, \\n Std: {np.std(xTestScaled, axis = 0)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "In tutto abbimo 30 attributi. Si considerino i primi due, che corrispondono ai seguenti attributi ['radius_mean', 'texture_mean']. \n",
    "\n",
    "Si generi lo scatterplot per le 2 features con le osservazioni del dataset di training. Disegnate con puntini rossi le osservazioni in cui la variabile target y è uguale a 1 (tumore maligno) e con punti blu le osservazioni in cui la variabile target y è uguale a -1 (tumore benigno).\n",
    "\n",
    "Guardando lo scatterplot appena generato, dite se le seguenti affermazioni sono vere o false\n",
    "- Sia data una variabile di test con 'radius_mean' = 3 e 'texture_mean'= 1. Un algoritmo di classificazione k-NN, con `k=11`, associa la variabile di test alla classe positiva.\n",
    "- Sia data una variabile di test con 'radius_mean' = -1 e 'texture_mean'= 2. Un algoritmo di classificazione k-NN, con `k=11`, associa la variabile di test alla classe positiva.\n",
    "-  Sia data una variabile di test con 'radius_mean' = 1 e 'texture_mean'= -1.1. Un algoritmo di classificazione k-NN, con `k=1`, associa la variabile di test alla classe positiva \n",
    "-  Sia data una variabile di test con 'radius_mean' = 1 e 'texture_mean'= -1.1. Un algoritmo di classificazione k-NN, con `k=5`, associa la variabile di test alla classe positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "marker": {
          "color": "rgba(0, 0, 255, .8)",
          "size": 10
         },
         "mode": "markers",
         "name": "tumore beigno",
         "type": "scatter",
         "uid": "2b49dae2-f731-11e8-8b32-309c2316f643",
         "x": [
          13.74,
          13.37,
          14.69,
          12.91,
          13.62,
          15,
          11.8,
          14.53,
          13.71,
          11.36,
          8.726,
          14.47,
          13.54,
          12.85,
          12.47,
          12.46,
          10.86,
          11.37,
          13.49,
          9.567,
          11.06,
          14.61,
          7.691,
          13.69,
          11.46,
          12.06,
          13.15,
          10.26,
          9.333,
          11.06,
          10.66,
          10.51,
          12.67,
          11.08,
          11.04,
          12.43,
          14.87,
          12.87,
          9.465,
          13.16,
          10.91,
          12.25,
          12.75,
          14.58,
          12.27,
          12.16,
          12.99,
          13.27,
          13.87,
          13.87,
          13.59,
          8.95,
          13.65,
          11.47,
          14.42,
          12.05,
          9.683,
          10.97,
          12.54,
          14.34,
          11.76,
          13.03,
          16.84,
          13.21,
          8.671,
          11.87,
          12.76,
          12.65,
          11.31,
          11.94,
          14.96,
          11.6,
          9.731,
          11.27,
          14.02,
          10.51,
          9.876,
          11.57,
          12.31,
          13.66,
          10.32,
          12.21,
          12.21,
          12.87,
          12.18,
          16.5,
          12.04,
          10.05,
          11.28,
          8.618,
          12.63,
          12.54,
          13,
          10.6,
          11.62,
          11.89,
          9,
          9.042,
          13.9,
          14.95,
          13.46,
          14.5,
          12.18,
          12.9,
          9.787,
          12.2,
          11.85,
          11.41,
          13.2,
          16.17,
          12.45,
          11.95,
          13.85,
          15.19,
          11.06,
          11.14,
          12.94,
          13.24,
          13.05,
          13.85,
          13.3,
          12.72,
          12,
          14.26,
          12.81,
          9.405,
          10.94,
          8.597,
          8.219,
          10.57,
          13.2,
          12.39,
          11.71,
          14.04,
          11.22,
          11.93,
          11.41,
          12.72,
          10.57,
          13.27,
          9.397,
          14.76,
          11.51,
          12.58,
          9.847,
          9.676,
          12.36,
          11.63,
          15.71,
          14.8,
          14.97,
          11.3,
          14.92,
          10.9,
          12.32,
          11.43,
          11.68,
          11.16,
          14.53,
          13.56,
          11.34,
          13.64,
          13.64,
          14.59,
          9.668,
          13.01,
          8.571,
          9.876,
          8.878,
          12.8,
          12.22,
          10.96,
          10.44,
          12.88,
          9.436,
          11.6,
          12.95,
          8.598,
          16.14,
          13.59,
          12.03,
          12.23,
          11.49,
          13.66,
          11.26,
          12.7,
          8.734,
          15.1,
          12.77,
          14.11,
          11.71,
          11.71,
          12.46,
          11.89,
          12.27,
          13.85,
          11.94,
          11.26,
          13.77,
          11.69,
          11.5,
          14.05,
          13.38,
          10.49,
          11.66,
          10.71,
          13.7,
          11.99,
          10.2,
          11.93,
          14.2,
          13.05,
          12.07,
          11.45,
          16.3,
          10.8,
          12.62,
          10.26,
          12.42,
          12.49,
          13.88,
          9.742,
          10.8,
          15.73,
          12.88,
          9.268,
          13.75,
          12.3,
          12.83,
          11.74,
          13.05,
          10.88,
          9.504,
          11.54,
          9.755,
          11.75,
          11.33,
          12.77,
          14.99,
          17.85,
          13.5,
          12.19,
          13,
          13.14,
          13.08,
          8.888,
          11.64,
          14.29,
          12.18
         ],
         "y": [
          17.91,
          16.39,
          13.98,
          16.33,
          23.23,
          15.51,
          17.26,
          13.98,
          18.68,
          17.57,
          15.83,
          24.99,
          14.36,
          21.37,
          17.31,
          19.89,
          21.48,
          18.89,
          22.3,
          15.91,
          14.83,
          15.69,
          25.44,
          16.07,
          18.16,
          18.9,
          15.34,
          16.58,
          21.94,
          17.12,
          15.15,
          20.19,
          17.3,
          14.71,
          14.93,
          17,
          20.21,
          16.21,
          21.01,
          20.54,
          12.35,
          22.44,
          16.7,
          13.66,
          29.97,
          18.03,
          14.23,
          14.76,
          20.7,
          16.21,
          17.84,
          15.76,
          13.16,
          16.03,
          16.54,
          14.63,
          19.34,
          17.2,
          16.32,
          13.47,
          21.6,
          18.42,
          19.46,
          28.06,
          14.45,
          21.54,
          13.37,
          18.17,
          19.04,
          20.76,
          19.1,
          18.36,
          15.34,
          15.5,
          15.66,
          23.09,
          19.4,
          19.04,
          16.52,
          19.13,
          16.35,
          14.09,
          18.02,
          19.54,
          14.08,
          18.29,
          28.14,
          17.53,
          13.39,
          11.79,
          20.76,
          18.07,
          20.78,
          18.95,
          18.18,
          21.17,
          14.4,
          18.9,
          19.24,
          18.77,
          28.21,
          10.89,
          17.84,
          15.92,
          19.94,
          15.21,
          17.46,
          14.92,
          17.43,
          16.07,
          16.41,
          14.96,
          17.21,
          13.21,
          14.96,
          14.07,
          16.17,
          20.13,
          18.59,
          19.6,
          21.57,
          13.78,
          28.23,
          19.65,
          13.06,
          21.7,
          18.59,
          18.6,
          20.7,
          18.32,
          15.82,
          17.48,
          16.67,
          15.98,
          33.81,
          21.53,
          10.82,
          17.67,
          20.22,
          17.02,
          21.68,
          14.74,
          23.93,
          18.4,
          15.68,
          13.14,
          18.54,
          29.29,
          13.93,
          17.66,
          16.95,
          18.19,
          14.93,
          12.96,
          12.39,
          17.31,
          16.17,
          21.41,
          19.34,
          13.9,
          18.61,
          16.34,
          15.6,
          22.68,
          18.1,
          22.22,
          13.1,
          17.27,
          15.49,
          17.46,
          20.04,
          17.62,
          15.46,
          18.22,
          18.32,
          12.84,
          16.02,
          20.98,
          14.86,
          21.84,
          17.93,
          19.56,
          14.59,
          15.15,
          19.96,
          12.17,
          16.84,
          16.39,
          29.43,
          12.88,
          15.45,
          17.19,
          12.83,
          18.35,
          17.92,
          15.18,
          18.24,
          19.83,
          13.27,
          24.44,
          18.45,
          27.15,
          30.72,
          18.61,
          17.07,
          20.39,
          17.64,
          24.89,
          17.48,
          10.91,
          20.53,
          13.84,
          13.44,
          20.97,
          15.7,
          21.98,
          17.15,
          14.71,
          15.04,
          16.85,
          16.16,
          15.67,
          9.71,
          11.28,
          28.92,
          12.87,
          23.77,
          15.9,
          15.73,
          14.69,
          19.31,
          15.62,
          12.44,
          10.72,
          28.2,
          20.18,
          14.16,
          21.41,
          22.11,
          13.23,
          12.71,
          13.29,
          25.13,
          20.74,
          15.71,
          14.64,
          18.33,
          16.82,
          20.52
         ]
        },
        {
         "marker": {
          "color": "rgba(255, 0, 0, .8)",
          "size": 10
         },
         "mode": "markers",
         "name": "tumore maligno",
         "type": "scatter",
         "uid": "2b49dae3-f731-11e8-8cc3-309c2316f643",
         "x": [
          17.35,
          16.11,
          18.49,
          13.71,
          15.46,
          19.21,
          18.81,
          17.14,
          19.07,
          19.16,
          19.8,
          13.61,
          11.84,
          14.71,
          16.65,
          16.13,
          21.75,
          14.27,
          15.06,
          27.22,
          21.16,
          20.16,
          12.45,
          18.65,
          15.61,
          14.68,
          11.42,
          17.06,
          14.58,
          12.83,
          20.48,
          14.86,
          17.47,
          12.34,
          20.09,
          17.95,
          16.69,
          16.35,
          14.25,
          17.3,
          20.59,
          16.27,
          17.29,
          18.03,
          11.76,
          14.99,
          19.18,
          11.8,
          25.73,
          24.63,
          17.46,
          19.44,
          20.47,
          20.18,
          24.25,
          23.29,
          20.34,
          20.44,
          18.82,
          21.71,
          15.75,
          18.22,
          19.53,
          19.79,
          15.05,
          19.59,
          20.64,
          17.08,
          20.51,
          23.27,
          18.22,
          18.08,
          15.66,
          18.01,
          19,
          17.91,
          13.17,
          14.78,
          16.24,
          18.46,
          18.31,
          19.17,
          16.74,
          15.3,
          20.26,
          15.75,
          16.25,
          10.95,
          15.46,
          15.49,
          19.4,
          19.45,
          20.29,
          19.55,
          14.9,
          13.61,
          17.42,
          13.43,
          13.86,
          13.17,
          17.02,
          15.5,
          18.45,
          18.61,
          17.05,
          20.31,
          19.59,
          18.31,
          14.95,
          14.25,
          12.77,
          13,
          12.68,
          19.53,
          13.73,
          17.68,
          13.44,
          11.08,
          18.77,
          23.21,
          17.01,
          28.11,
          27.42,
          16.6,
          17.54,
          19.19,
          13.28,
          14.22,
          15.53,
          13.11,
          19.55,
          20.57,
          15.12,
          16.13,
          20.2,
          15.37,
          23.51,
          15.85,
          20.92,
          19.73,
          14.6,
          19.68,
          14.42,
          21.37,
          19.02,
          16.03,
          14.19,
          18.66,
          13.98
         ],
         "y": [
          23.06,
          18.05,
          17.52,
          20.83,
          11.89,
          18.57,
          19.98,
          16.4,
          24.81,
          26.6,
          21.56,
          24.69,
          18.7,
          21.59,
          21.38,
          20.68,
          20.99,
          22.55,
          19.83,
          21.87,
          23.04,
          19.66,
          15.7,
          17.6,
          19.38,
          20.13,
          20.38,
          21,
          21.53,
          22.33,
          21.46,
          23.21,
          24.68,
          26.86,
          23.86,
          20.01,
          20.2,
          23.29,
          21.72,
          17.08,
          21.24,
          20.71,
          22.13,
          16.85,
          18.14,
          25.2,
          22.49,
          16.58,
          17.46,
          21.6,
          39.28,
          18.82,
          20.67,
          19.54,
          20.2,
          26.67,
          21.51,
          21.78,
          21.97,
          17.25,
          20.25,
          18.7,
          32.47,
          25.12,
          19.07,
          25,
          17.35,
          27.15,
          27.81,
          22.04,
          18.87,
          21.84,
          23.2,
          20.56,
          18.91,
          21.02,
          21.81,
          23.94,
          18.77,
          18.52,
          18.58,
          24.8,
          21.59,
          25.27,
          23.03,
          19.22,
          19.51,
          21.35,
          23.95,
          19.97,
          18.18,
          19.33,
          14.34,
          28.77,
          22.53,
          24.98,
          25.56,
          19.63,
          16.93,
          18.66,
          23.98,
          21.08,
          21.91,
          20.25,
          19.08,
          27.06,
          18.15,
          20.58,
          17.57,
          22.15,
          22.47,
          21.82,
          23.84,
          18.9,
          22.61,
          20.74,
          21.58,
          18.83,
          21.43,
          26.97,
          20.26,
          18.47,
          26.27,
          28.08,
          19.32,
          15.94,
          20.28,
          23.12,
          33.56,
          15.56,
          23.21,
          17.77,
          16.68,
          17.88,
          26.83,
          22.76,
          24.27,
          23.95,
          25.09,
          19.82,
          23.29,
          21.68,
          19.77,
          15.1,
          24.59,
          15.51,
          23.81,
          17.12,
          19.62
         ]
        }
       ],
       "layout": {
        "title": "Osservazione dei tumori",
        "xaxis": {
         "title": "radius_mean"
        },
        "yaxis": {
         "title": "texture_mean"
        }
       }
      },
      "text/html": [
       "<div id=\"afefb7be-e1d9-48b9-ba16-cf6e324a514f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"afefb7be-e1d9-48b9-ba16-cf6e324a514f\", [{\"marker\": {\"color\": \"rgba(0, 0, 255, .8)\", \"size\": 10}, \"mode\": \"markers\", \"name\": \"tumore beigno\", \"x\": [13.74, 13.37, 14.69, 12.91, 13.62, 15.0, 11.8, 14.53, 13.71, 11.36, 8.726, 14.47, 13.54, 12.85, 12.47, 12.46, 10.86, 11.37, 13.49, 9.567, 11.06, 14.61, 7.691, 13.69, 11.46, 12.06, 13.15, 10.26, 9.333, 11.06, 10.66, 10.51, 12.67, 11.08, 11.04, 12.43, 14.87, 12.87, 9.465, 13.16, 10.91, 12.25, 12.75, 14.58, 12.27, 12.16, 12.99, 13.27, 13.87, 13.87, 13.59, 8.95, 13.65, 11.47, 14.42, 12.05, 9.683, 10.97, 12.54, 14.34, 11.76, 13.03, 16.84, 13.21, 8.671, 11.87, 12.76, 12.65, 11.31, 11.94, 14.96, 11.6, 9.731, 11.27, 14.02, 10.51, 9.876, 11.57, 12.31, 13.66, 10.32, 12.21, 12.21, 12.87, 12.18, 16.5, 12.04, 10.05, 11.28, 8.618, 12.63, 12.54, 13.0, 10.6, 11.62, 11.89, 9.0, 9.042, 13.9, 14.95, 13.46, 14.5, 12.18, 12.9, 9.787, 12.2, 11.85, 11.41, 13.2, 16.17, 12.45, 11.95, 13.85, 15.19, 11.06, 11.14, 12.94, 13.24, 13.05, 13.85, 13.3, 12.72, 12.0, 14.26, 12.81, 9.405, 10.94, 8.597, 8.219, 10.57, 13.2, 12.39, 11.71, 14.04, 11.22, 11.93, 11.41, 12.72, 10.57, 13.27, 9.397, 14.76, 11.51, 12.58, 9.847, 9.676, 12.36, 11.63, 15.71, 14.8, 14.97, 11.3, 14.92, 10.9, 12.32, 11.43, 11.68, 11.16, 14.53, 13.56, 11.34, 13.64, 13.64, 14.59, 9.668, 13.01, 8.571, 9.876, 8.878, 12.8, 12.22, 10.96, 10.44, 12.88, 9.436, 11.6, 12.95, 8.598, 16.14, 13.59, 12.03, 12.23, 11.49, 13.66, 11.26, 12.7, 8.734, 15.1, 12.77, 14.11, 11.71, 11.71, 12.46, 11.89, 12.27, 13.85, 11.94, 11.26, 13.77, 11.69, 11.5, 14.05, 13.38, 10.49, 11.66, 10.71, 13.7, 11.99, 10.2, 11.93, 14.2, 13.05, 12.07, 11.45, 16.3, 10.8, 12.62, 10.26, 12.42, 12.49, 13.88, 9.742, 10.8, 15.73, 12.88, 9.268, 13.75, 12.3, 12.83, 11.74, 13.05, 10.88, 9.504, 11.54, 9.755, 11.75, 11.33, 12.77, 14.99, 17.85, 13.5, 12.19, 13.0, 13.14, 13.08, 8.888, 11.64, 14.29, 12.18], \"y\": [17.91, 16.39, 13.98, 16.33, 23.23, 15.51, 17.26, 13.98, 18.68, 17.57, 15.83, 24.99, 14.36, 21.37, 17.31, 19.89, 21.48, 18.89, 22.3, 15.91, 14.83, 15.69, 25.44, 16.07, 18.16, 18.9, 15.34, 16.58, 21.94, 17.12, 15.15, 20.19, 17.3, 14.71, 14.93, 17.0, 20.21, 16.21, 21.01, 20.54, 12.35, 22.44, 16.7, 13.66, 29.97, 18.03, 14.23, 14.76, 20.7, 16.21, 17.84, 15.76, 13.16, 16.03, 16.54, 14.63, 19.34, 17.2, 16.32, 13.47, 21.6, 18.42, 19.46, 28.06, 14.45, 21.54, 13.37, 18.17, 19.04, 20.76, 19.1, 18.36, 15.34, 15.5, 15.66, 23.09, 19.4, 19.04, 16.52, 19.13, 16.35, 14.09, 18.02, 19.54, 14.08, 18.29, 28.14, 17.53, 13.39, 11.79, 20.76, 18.07, 20.78, 18.95, 18.18, 21.17, 14.4, 18.9, 19.24, 18.77, 28.21, 10.89, 17.84, 15.92, 19.94, 15.21, 17.46, 14.92, 17.43, 16.07, 16.41, 14.96, 17.21, 13.21, 14.96, 14.07, 16.17, 20.13, 18.59, 19.6, 21.57, 13.78, 28.23, 19.65, 13.06, 21.7, 18.59, 18.6, 20.7, 18.32, 15.82, 17.48, 16.67, 15.98, 33.81, 21.53, 10.82, 17.67, 20.22, 17.02, 21.68, 14.74, 23.93, 18.4, 15.68, 13.14, 18.54, 29.29, 13.93, 17.66, 16.95, 18.19, 14.93, 12.96, 12.39, 17.31, 16.17, 21.41, 19.34, 13.9, 18.61, 16.34, 15.6, 22.68, 18.1, 22.22, 13.1, 17.27, 15.49, 17.46, 20.04, 17.62, 15.46, 18.22, 18.32, 12.84, 16.02, 20.98, 14.86, 21.84, 17.93, 19.56, 14.59, 15.15, 19.96, 12.17, 16.84, 16.39, 29.43, 12.88, 15.45, 17.19, 12.83, 18.35, 17.92, 15.18, 18.24, 19.83, 13.27, 24.44, 18.45, 27.15, 30.72, 18.61, 17.07, 20.39, 17.64, 24.89, 17.48, 10.91, 20.53, 13.84, 13.44, 20.97, 15.7, 21.98, 17.15, 14.71, 15.04, 16.85, 16.16, 15.67, 9.71, 11.28, 28.92, 12.87, 23.77, 15.9, 15.73, 14.69, 19.31, 15.62, 12.44, 10.72, 28.2, 20.18, 14.16, 21.41, 22.11, 13.23, 12.71, 13.29, 25.13, 20.74, 15.71, 14.64, 18.33, 16.82, 20.52], \"type\": \"scatter\", \"uid\": \"2b49dae2-f731-11e8-8b32-309c2316f643\"}, {\"marker\": {\"color\": \"rgba(255, 0, 0, .8)\", \"size\": 10}, \"mode\": \"markers\", \"name\": \"tumore maligno\", \"x\": [17.35, 16.11, 18.49, 13.71, 15.46, 19.21, 18.81, 17.14, 19.07, 19.16, 19.8, 13.61, 11.84, 14.71, 16.65, 16.13, 21.75, 14.27, 15.06, 27.22, 21.16, 20.16, 12.45, 18.65, 15.61, 14.68, 11.42, 17.06, 14.58, 12.83, 20.48, 14.86, 17.47, 12.34, 20.09, 17.95, 16.69, 16.35, 14.25, 17.3, 20.59, 16.27, 17.29, 18.03, 11.76, 14.99, 19.18, 11.8, 25.73, 24.63, 17.46, 19.44, 20.47, 20.18, 24.25, 23.29, 20.34, 20.44, 18.82, 21.71, 15.75, 18.22, 19.53, 19.79, 15.05, 19.59, 20.64, 17.08, 20.51, 23.27, 18.22, 18.08, 15.66, 18.01, 19.0, 17.91, 13.17, 14.78, 16.24, 18.46, 18.31, 19.17, 16.74, 15.3, 20.26, 15.75, 16.25, 10.95, 15.46, 15.49, 19.4, 19.45, 20.29, 19.55, 14.9, 13.61, 17.42, 13.43, 13.86, 13.17, 17.02, 15.5, 18.45, 18.61, 17.05, 20.31, 19.59, 18.31, 14.95, 14.25, 12.77, 13.0, 12.68, 19.53, 13.73, 17.68, 13.44, 11.08, 18.77, 23.21, 17.01, 28.11, 27.42, 16.6, 17.54, 19.19, 13.28, 14.22, 15.53, 13.11, 19.55, 20.57, 15.12, 16.13, 20.2, 15.37, 23.51, 15.85, 20.92, 19.73, 14.6, 19.68, 14.42, 21.37, 19.02, 16.03, 14.19, 18.66, 13.98], \"y\": [23.06, 18.05, 17.52, 20.83, 11.89, 18.57, 19.98, 16.4, 24.81, 26.6, 21.56, 24.69, 18.7, 21.59, 21.38, 20.68, 20.99, 22.55, 19.83, 21.87, 23.04, 19.66, 15.7, 17.6, 19.38, 20.13, 20.38, 21.0, 21.53, 22.33, 21.46, 23.21, 24.68, 26.86, 23.86, 20.01, 20.2, 23.29, 21.72, 17.08, 21.24, 20.71, 22.13, 16.85, 18.14, 25.2, 22.49, 16.58, 17.46, 21.6, 39.28, 18.82, 20.67, 19.54, 20.2, 26.67, 21.51, 21.78, 21.97, 17.25, 20.25, 18.7, 32.47, 25.12, 19.07, 25.0, 17.35, 27.15, 27.81, 22.04, 18.87, 21.84, 23.2, 20.56, 18.91, 21.02, 21.81, 23.94, 18.77, 18.52, 18.58, 24.8, 21.59, 25.27, 23.03, 19.22, 19.51, 21.35, 23.95, 19.97, 18.18, 19.33, 14.34, 28.77, 22.53, 24.98, 25.56, 19.63, 16.93, 18.66, 23.98, 21.08, 21.91, 20.25, 19.08, 27.06, 18.15, 20.58, 17.57, 22.15, 22.47, 21.82, 23.84, 18.9, 22.61, 20.74, 21.58, 18.83, 21.43, 26.97, 20.26, 18.47, 26.27, 28.08, 19.32, 15.94, 20.28, 23.12, 33.56, 15.56, 23.21, 17.77, 16.68, 17.88, 26.83, 22.76, 24.27, 23.95, 25.09, 19.82, 23.29, 21.68, 19.77, 15.1, 24.59, 15.51, 23.81, 17.12, 19.62], \"type\": \"scatter\", \"uid\": \"2b49dae3-f731-11e8-8cc3-309c2316f643\"}], {\"title\": \"Osservazione dei tumori\", \"xaxis\": {\"title\": \"radius_mean\"}, \"yaxis\": {\"title\": \"texture_mean\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"afefb7be-e1d9-48b9-ba16-cf6e324a514f\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"afefb7be-e1d9-48b9-ba16-cf6e324a514f\", [{\"marker\": {\"color\": \"rgba(0, 0, 255, .8)\", \"size\": 10}, \"mode\": \"markers\", \"name\": \"tumore beigno\", \"x\": [13.74, 13.37, 14.69, 12.91, 13.62, 15.0, 11.8, 14.53, 13.71, 11.36, 8.726, 14.47, 13.54, 12.85, 12.47, 12.46, 10.86, 11.37, 13.49, 9.567, 11.06, 14.61, 7.691, 13.69, 11.46, 12.06, 13.15, 10.26, 9.333, 11.06, 10.66, 10.51, 12.67, 11.08, 11.04, 12.43, 14.87, 12.87, 9.465, 13.16, 10.91, 12.25, 12.75, 14.58, 12.27, 12.16, 12.99, 13.27, 13.87, 13.87, 13.59, 8.95, 13.65, 11.47, 14.42, 12.05, 9.683, 10.97, 12.54, 14.34, 11.76, 13.03, 16.84, 13.21, 8.671, 11.87, 12.76, 12.65, 11.31, 11.94, 14.96, 11.6, 9.731, 11.27, 14.02, 10.51, 9.876, 11.57, 12.31, 13.66, 10.32, 12.21, 12.21, 12.87, 12.18, 16.5, 12.04, 10.05, 11.28, 8.618, 12.63, 12.54, 13.0, 10.6, 11.62, 11.89, 9.0, 9.042, 13.9, 14.95, 13.46, 14.5, 12.18, 12.9, 9.787, 12.2, 11.85, 11.41, 13.2, 16.17, 12.45, 11.95, 13.85, 15.19, 11.06, 11.14, 12.94, 13.24, 13.05, 13.85, 13.3, 12.72, 12.0, 14.26, 12.81, 9.405, 10.94, 8.597, 8.219, 10.57, 13.2, 12.39, 11.71, 14.04, 11.22, 11.93, 11.41, 12.72, 10.57, 13.27, 9.397, 14.76, 11.51, 12.58, 9.847, 9.676, 12.36, 11.63, 15.71, 14.8, 14.97, 11.3, 14.92, 10.9, 12.32, 11.43, 11.68, 11.16, 14.53, 13.56, 11.34, 13.64, 13.64, 14.59, 9.668, 13.01, 8.571, 9.876, 8.878, 12.8, 12.22, 10.96, 10.44, 12.88, 9.436, 11.6, 12.95, 8.598, 16.14, 13.59, 12.03, 12.23, 11.49, 13.66, 11.26, 12.7, 8.734, 15.1, 12.77, 14.11, 11.71, 11.71, 12.46, 11.89, 12.27, 13.85, 11.94, 11.26, 13.77, 11.69, 11.5, 14.05, 13.38, 10.49, 11.66, 10.71, 13.7, 11.99, 10.2, 11.93, 14.2, 13.05, 12.07, 11.45, 16.3, 10.8, 12.62, 10.26, 12.42, 12.49, 13.88, 9.742, 10.8, 15.73, 12.88, 9.268, 13.75, 12.3, 12.83, 11.74, 13.05, 10.88, 9.504, 11.54, 9.755, 11.75, 11.33, 12.77, 14.99, 17.85, 13.5, 12.19, 13.0, 13.14, 13.08, 8.888, 11.64, 14.29, 12.18], \"y\": [17.91, 16.39, 13.98, 16.33, 23.23, 15.51, 17.26, 13.98, 18.68, 17.57, 15.83, 24.99, 14.36, 21.37, 17.31, 19.89, 21.48, 18.89, 22.3, 15.91, 14.83, 15.69, 25.44, 16.07, 18.16, 18.9, 15.34, 16.58, 21.94, 17.12, 15.15, 20.19, 17.3, 14.71, 14.93, 17.0, 20.21, 16.21, 21.01, 20.54, 12.35, 22.44, 16.7, 13.66, 29.97, 18.03, 14.23, 14.76, 20.7, 16.21, 17.84, 15.76, 13.16, 16.03, 16.54, 14.63, 19.34, 17.2, 16.32, 13.47, 21.6, 18.42, 19.46, 28.06, 14.45, 21.54, 13.37, 18.17, 19.04, 20.76, 19.1, 18.36, 15.34, 15.5, 15.66, 23.09, 19.4, 19.04, 16.52, 19.13, 16.35, 14.09, 18.02, 19.54, 14.08, 18.29, 28.14, 17.53, 13.39, 11.79, 20.76, 18.07, 20.78, 18.95, 18.18, 21.17, 14.4, 18.9, 19.24, 18.77, 28.21, 10.89, 17.84, 15.92, 19.94, 15.21, 17.46, 14.92, 17.43, 16.07, 16.41, 14.96, 17.21, 13.21, 14.96, 14.07, 16.17, 20.13, 18.59, 19.6, 21.57, 13.78, 28.23, 19.65, 13.06, 21.7, 18.59, 18.6, 20.7, 18.32, 15.82, 17.48, 16.67, 15.98, 33.81, 21.53, 10.82, 17.67, 20.22, 17.02, 21.68, 14.74, 23.93, 18.4, 15.68, 13.14, 18.54, 29.29, 13.93, 17.66, 16.95, 18.19, 14.93, 12.96, 12.39, 17.31, 16.17, 21.41, 19.34, 13.9, 18.61, 16.34, 15.6, 22.68, 18.1, 22.22, 13.1, 17.27, 15.49, 17.46, 20.04, 17.62, 15.46, 18.22, 18.32, 12.84, 16.02, 20.98, 14.86, 21.84, 17.93, 19.56, 14.59, 15.15, 19.96, 12.17, 16.84, 16.39, 29.43, 12.88, 15.45, 17.19, 12.83, 18.35, 17.92, 15.18, 18.24, 19.83, 13.27, 24.44, 18.45, 27.15, 30.72, 18.61, 17.07, 20.39, 17.64, 24.89, 17.48, 10.91, 20.53, 13.84, 13.44, 20.97, 15.7, 21.98, 17.15, 14.71, 15.04, 16.85, 16.16, 15.67, 9.71, 11.28, 28.92, 12.87, 23.77, 15.9, 15.73, 14.69, 19.31, 15.62, 12.44, 10.72, 28.2, 20.18, 14.16, 21.41, 22.11, 13.23, 12.71, 13.29, 25.13, 20.74, 15.71, 14.64, 18.33, 16.82, 20.52], \"type\": \"scatter\", \"uid\": \"2b49dae2-f731-11e8-8b32-309c2316f643\"}, {\"marker\": {\"color\": \"rgba(255, 0, 0, .8)\", \"size\": 10}, \"mode\": \"markers\", \"name\": \"tumore maligno\", \"x\": [17.35, 16.11, 18.49, 13.71, 15.46, 19.21, 18.81, 17.14, 19.07, 19.16, 19.8, 13.61, 11.84, 14.71, 16.65, 16.13, 21.75, 14.27, 15.06, 27.22, 21.16, 20.16, 12.45, 18.65, 15.61, 14.68, 11.42, 17.06, 14.58, 12.83, 20.48, 14.86, 17.47, 12.34, 20.09, 17.95, 16.69, 16.35, 14.25, 17.3, 20.59, 16.27, 17.29, 18.03, 11.76, 14.99, 19.18, 11.8, 25.73, 24.63, 17.46, 19.44, 20.47, 20.18, 24.25, 23.29, 20.34, 20.44, 18.82, 21.71, 15.75, 18.22, 19.53, 19.79, 15.05, 19.59, 20.64, 17.08, 20.51, 23.27, 18.22, 18.08, 15.66, 18.01, 19.0, 17.91, 13.17, 14.78, 16.24, 18.46, 18.31, 19.17, 16.74, 15.3, 20.26, 15.75, 16.25, 10.95, 15.46, 15.49, 19.4, 19.45, 20.29, 19.55, 14.9, 13.61, 17.42, 13.43, 13.86, 13.17, 17.02, 15.5, 18.45, 18.61, 17.05, 20.31, 19.59, 18.31, 14.95, 14.25, 12.77, 13.0, 12.68, 19.53, 13.73, 17.68, 13.44, 11.08, 18.77, 23.21, 17.01, 28.11, 27.42, 16.6, 17.54, 19.19, 13.28, 14.22, 15.53, 13.11, 19.55, 20.57, 15.12, 16.13, 20.2, 15.37, 23.51, 15.85, 20.92, 19.73, 14.6, 19.68, 14.42, 21.37, 19.02, 16.03, 14.19, 18.66, 13.98], \"y\": [23.06, 18.05, 17.52, 20.83, 11.89, 18.57, 19.98, 16.4, 24.81, 26.6, 21.56, 24.69, 18.7, 21.59, 21.38, 20.68, 20.99, 22.55, 19.83, 21.87, 23.04, 19.66, 15.7, 17.6, 19.38, 20.13, 20.38, 21.0, 21.53, 22.33, 21.46, 23.21, 24.68, 26.86, 23.86, 20.01, 20.2, 23.29, 21.72, 17.08, 21.24, 20.71, 22.13, 16.85, 18.14, 25.2, 22.49, 16.58, 17.46, 21.6, 39.28, 18.82, 20.67, 19.54, 20.2, 26.67, 21.51, 21.78, 21.97, 17.25, 20.25, 18.7, 32.47, 25.12, 19.07, 25.0, 17.35, 27.15, 27.81, 22.04, 18.87, 21.84, 23.2, 20.56, 18.91, 21.02, 21.81, 23.94, 18.77, 18.52, 18.58, 24.8, 21.59, 25.27, 23.03, 19.22, 19.51, 21.35, 23.95, 19.97, 18.18, 19.33, 14.34, 28.77, 22.53, 24.98, 25.56, 19.63, 16.93, 18.66, 23.98, 21.08, 21.91, 20.25, 19.08, 27.06, 18.15, 20.58, 17.57, 22.15, 22.47, 21.82, 23.84, 18.9, 22.61, 20.74, 21.58, 18.83, 21.43, 26.97, 20.26, 18.47, 26.27, 28.08, 19.32, 15.94, 20.28, 23.12, 33.56, 15.56, 23.21, 17.77, 16.68, 17.88, 26.83, 22.76, 24.27, 23.95, 25.09, 19.82, 23.29, 21.68, 19.77, 15.1, 24.59, 15.51, 23.81, 17.12, 19.62], \"type\": \"scatter\", \"uid\": \"2b49dae3-f731-11e8-8cc3-309c2316f643\"}], {\"title\": \"Osservazione dei tumori\", \"xaxis\": {\"title\": \"radius_mean\"}, \"yaxis\": {\"title\": \"texture_mean\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution (to be completed)\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected = True)\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = xTrain[yTrain==1,0],\n",
    "    y = xTrain[yTrain==1,1],\n",
    "    name=\"tumore maligno\",\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(255, 0, 0, .8)',\n",
    "\n",
    "    )\n",
    ")\n",
    "trace0 = go.Scatter(\n",
    "    x = xTrain[yTrain== -1,0],\n",
    "    y = xTrain[yTrain==-1,1],\n",
    "    name=\"tumore beigno\",\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgba(0, 0, 255, .8)',\n",
    "\n",
    "    )\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "layout = go.Layout( title = 'Osservazione dei tumori',\n",
    "                    xaxis = {'title': 'radius_mean'},\n",
    "                    yaxis = {'title': 'texture_mean'})\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "In tutti e quattro i casi la classe sarebbe sempre tumore benigno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "### 1.3.1 \n",
    "Scrivere una funzione ``y_hat = knn(xTrainScaled,yTrain,xTestScaled,k,th)`` che implementa l'algoritmo di classificazione k-NN. \n",
    "\n",
    "La funzione riceve come parametri di ingress0:\n",
    "- ``xTrainScaled``: array di 2 dimensioni contente gli attributi (già normalizzati) delle osservazioni del training set. Numero di righe: m (numero di osservazioni), numero di colonne: n (numero degli attributi considerati).\n",
    "- ``yTrain``: array di 1 dimensione contente le m osservazioni della variabile target del training set\n",
    "- ``xTestScaled``: array di 2 dimensioni contente gli attributi (già normalizzati) delle osservazioni del test set di cui si vuole stimare la classe. Numero di righe: t (numero di osservazioni del test set), numero di colonne: n (numero degli attributi considerati).\n",
    "- ``k``: numero intero dispari. Parametro dell'algoritmo k-NN. \n",
    "- ``th``: soglia (o threshould in inglese). Numero intero positivo compreso tra 0 e 1. E' il valore soglia dell'algoritmo. La variabile di test è attributa alla classe positiva se lo ``score`` della classe positiva è maggiore o uguale a ``th``. Data un'istanza di test, lo score della classe positiva  è uguale a $p/k$, dove $p$ è il numero di istanze positive tra le k istanze piu' vicine. Esempio per k=9. Data un'istanza di test, se tra le 9 istanze di training piu' vicine ci sono 5 istanze positive, allora p=5 e lo score è uguale a = 5/9.   Se usate un criterio di maggioranza (come visto a lezione), allora ``th = 0.5``.\n",
    "\n",
    "La funzione restituisce:\n",
    "- ``y_hat``: array di 1 dimensione contente la stima della classe delle t osservazioni del test set\n",
    "- ``y_score``: array di 1 dimensione contente la score (della classe positiva) per ogni osservazione del test set. \n",
    "\n",
    "Valutate il comportamento della funzione appena creata sui dati di test per k=9 e th = 0.5. Usate sempre i primi 2 attributi (radius_mean e texture_mean). Quale è l'accuratezza del vostro classificatore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza: 0.9005847953216374\n"
     ]
    }
   ],
   "source": [
    "# Solution 1.3.1 (to be completed)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn(xTrainScaled, yTrain, xTestScaled, k, th):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Valuta dimensione del set di training\n",
    "    if np.ndim(xTrainScaled)>1:\n",
    "        (m,n) = np.shape(xTrainScaled)\n",
    "    else:\n",
    "        n = 1\n",
    "        m = np.shape(xTrainScaled)[0]\n",
    "        np.reshape(xTrainScaled,(m,1)) # Fai un reshape per avere array di dimension 2\n",
    "        \n",
    "    t = np.shape(xTestScaled)[0] # Dimensione del set di training\n",
    "\n",
    "    y_hat = np.zeros(t)\n",
    "    y_score = np.zeros(t) \n",
    "    \n",
    "    clf.fit(xTrainScaled, yTrain)\n",
    "    clf.predict(xTestScaled)\n",
    "    y_score = clf.predict_proba(xTestScaled)[:,1]\n",
    "    \n",
    "    b = th <= y_score\n",
    "    \n",
    "    for i in range(t):\n",
    "        if(b[i] == True):\n",
    "            y_hat[i] = 1\n",
    "        else:\n",
    "            y_hat[i] = -1\n",
    "    \n",
    "    return y_hat, y_score\n",
    "\n",
    "\n",
    "y_hat, y_score = knn(xTrainScaled[:,0:2], yTrain, xTestScaled[:,0:2], k=9, th=0.5)\n",
    "accuracy = np.mean(yTest == y_hat)\n",
    "print(f\"Accuratezza: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  \n",
    "Scrivere una funzione ``accuracy, TP, FP, FN, TN, TPR, FPR = evaluatekNN(yTest,y_hat)`` che, dato l'array con la classe vera ``yTest``delle osservazioni di test e la classe stimata ``y_hat`` dall'algoritmo kNN, restituisce:\n",
    "- accuary: accuratezza dell'algoritmo\n",
    "- TP, FP, FN, TN, TPR, FPR: True Positive, Falso Positive, False Negative, True Negative, True Positive Rate, False Positive Rate.\n",
    "\n",
    "Valutate il comportamento della funzione appena creata usando la predizione della classe y_hat calcolata al passo precedente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9005847953216374; TPR = 0.8095238095238095; FPR = 0.046296296296296294; ....... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution 1.3.2 (to be completed)\n",
    "\n",
    "def evaluatekNN(yTest,y_hat):\n",
    "\n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    for i in range (len(yTest)):            \n",
    "        if (yTest[i]==y_hat[i]):\n",
    "            if(yTest[i]>0):\n",
    "                TP+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "        else:\n",
    "            if(yTest[i]<0):\n",
    "                FP+=1\n",
    "            else:\n",
    "                FN+=1\n",
    "\n",
    "    TPR=TP/(TP+FN)\n",
    "    FPR=FP/(FP+TN)\n",
    "    accuracy=(TP+TN)/len(yTest)\n",
    "    \n",
    "    return accuracy, TP, FP, FN, TN, TPR, FPR\n",
    "\n",
    "accuracy, TP, FP, FN, TN, TPR, FPR = evaluatekNN(yTest,y_hat)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}; TPR = {TPR}; FPR = {FPR}; ....... \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 \n",
    "\n",
    "- In un'applicazione diagnostica come quella che stiamo considerando in questo esercizio, è meglio avere un TPR alto (a discapito di un FPR alto) o un TPR basso (col vantaggio di avere un FPR basso)?\n",
    "\n",
    "- Come posso fare ad aumentare (o diminuire) il TPR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "A dipendenza della soluzione da intraprendere e dai suoi costi è meglio uno o l'altro. Se abbiamo i costi alti è meglio un TPR basso altrimenti se i costi sono bassi allora un TPR alto fa al caso nostro.\n",
    "\n",
    "Per aumentare o diminuire i TPR basta semplicemente cambiare la nostra k e la nostra soglia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4\n",
    "Disegnate la curva ROC (variando il parametro di soglia `th`) per $k=5$, $k=9$ e $k=m$ (dove m è la lunghezza del set di training). Si consideri sempre i primi 2 attributi. \n",
    "\n",
    "- Cosa succede se ``th=0``? E se ``th=1``?\n",
    "\n",
    "- Calcolare la AUC (area under curve) delle 3 ROC disegnate sopra: comando per calcolare la AUC:  ``roc_auc_score(yTest,y_score)``\n",
    "\n",
    "- (opzionale) Sapete spiegare perchè per $k=m$ il classificatore si comporta sempre come il baseline (dummy) classifier?\n",
    "\n",
    "- Quale dei 3 classificatori scegliete? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 5: AUC: 0.951058201058201\n",
      "k = 9: AUC: 0.9544385655496767\n",
      "k = 398: AUC: 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "markers+lines",
         "name": "k-NN classifier for k=5",
         "text": [
          0,
          0.2,
          0.4,
          0.6000000000000001,
          0.8,
          1
         ],
         "type": "scatter",
         "uid": "2f9d2ed0-f731-11e8-90fd-309c2316f643",
         "x": [
          1,
          0.37037037037037035,
          0.08333333333333333,
          0.018518518518518517,
          0.018518518518518517,
          0
         ],
         "y": [
          1,
          0.9682539682539683,
          0.9047619047619048,
          0.6984126984126984,
          0.6984126984126984,
          0.5714285714285714
         ]
        },
        {
         "mode": "markers+lines",
         "name": "k-NN classifier for k=9",
         "text": [
          0,
          0.1111111111111111,
          0.2222222222222222,
          0.3333333333333333,
          0.4444444444444444,
          0.5555555555555556,
          0.6666666666666666,
          0.7777777777777777,
          0.8888888888888888,
          1
         ],
         "type": "scatter",
         "uid": "2f9d4236-f731-11e8-b8d1-309c2316f643",
         "x": [
          1,
          0.5,
          0.23148148148148148,
          0.1111111111111111,
          0.09259259259259259,
          0.046296296296296294,
          0.027777777777777776,
          0.018518518518518517,
          0.018518518518518517,
          0
         ],
         "y": [
          1,
          0.9841269841269841,
          0.9523809523809523,
          0.9047619047619048,
          0.8571428571428571,
          0.8095238095238095,
          0.746031746031746,
          0.7142857142857143,
          0.6031746031746031,
          0.5396825396825397
         ]
        },
        {
         "mode": "markers+lines",
         "name": "k-NN classifier for k=398",
         "text": [
          0,
          0.002512562814070352,
          0.005025125628140704,
          0.007537688442211055,
          0.010050251256281407,
          0.01256281407035176,
          0.01507537688442211,
          0.017587939698492462,
          0.020100502512562814,
          0.022613065326633167,
          0.02512562814070352,
          0.02763819095477387,
          0.03015075376884422,
          0.032663316582914576,
          0.035175879396984924,
          0.03768844221105528,
          0.04020100502512563,
          0.04271356783919598,
          0.04522613065326633,
          0.04773869346733668,
          0.05025125628140704,
          0.052763819095477386,
          0.05527638190954774,
          0.05778894472361809,
          0.06030150753768844,
          0.06281407035175879,
          0.06532663316582915,
          0.0678391959798995,
          0.07035175879396985,
          0.0728643216080402,
          0.07537688442211056,
          0.07788944723618091,
          0.08040201005025126,
          0.0829145728643216,
          0.08542713567839195,
          0.08793969849246232,
          0.09045226130653267,
          0.09296482412060302,
          0.09547738693467336,
          0.09798994974874373,
          0.10050251256281408,
          0.10301507537688442,
          0.10552763819095477,
          0.10804020100502512,
          0.11055276381909548,
          0.11306532663316583,
          0.11557788944723618,
          0.11809045226130653,
          0.12060301507537688,
          0.12311557788944724,
          0.12562814070351758,
          0.12814070351758794,
          0.1306532663316583,
          0.13316582914572864,
          0.135678391959799,
          0.13819095477386936,
          0.1407035175879397,
          0.14321608040201006,
          0.1457286432160804,
          0.14824120603015076,
          0.15075376884422112,
          0.15326633165829145,
          0.15577889447236182,
          0.15829145728643215,
          0.16080402010050251,
          0.16331658291457288,
          0.1658291457286432,
          0.16834170854271358,
          0.1708542713567839,
          0.17336683417085427,
          0.17587939698492464,
          0.17839195979899497,
          0.18090452261306533,
          0.18341708542713567,
          0.18592964824120603,
          0.1884422110552764,
          0.19095477386934673,
          0.1934673366834171,
          0.19597989949748745,
          0.1984924623115578,
          0.20100502512562815,
          0.20351758793969849,
          0.20603015075376885,
          0.2085427135678392,
          0.21105527638190955,
          0.2135678391959799,
          0.21608040201005024,
          0.2185929648241206,
          0.22110552763819097,
          0.2236180904522613,
          0.22613065326633167,
          0.228643216080402,
          0.23115577889447236,
          0.23366834170854273,
          0.23618090452261306,
          0.23869346733668342,
          0.24120603015075376,
          0.24371859296482412,
          0.24623115577889448,
          0.24874371859296482,
          0.25125628140703515,
          0.2537688442211055,
          0.2562814070351759,
          0.25879396984924624,
          0.2613065326633166,
          0.2638190954773869,
          0.2663316582914573,
          0.26884422110552764,
          0.271356783919598,
          0.27386934673366836,
          0.2763819095477387,
          0.27889447236180903,
          0.2814070351758794,
          0.28391959798994976,
          0.2864321608040201,
          0.2889447236180905,
          0.2914572864321608,
          0.29396984924623115,
          0.2964824120603015,
          0.2989949748743719,
          0.30150753768844224,
          0.30402010050251255,
          0.3065326633165829,
          0.30904522613065327,
          0.31155778894472363,
          0.314070351758794,
          0.3165829145728643,
          0.31909547738693467,
          0.32160804020100503,
          0.3241206030150754,
          0.32663316582914576,
          0.32914572864321606,
          0.3316582914572864,
          0.3341708542713568,
          0.33668341708542715,
          0.3391959798994975,
          0.3417085427135678,
          0.3442211055276382,
          0.34673366834170855,
          0.3492462311557789,
          0.35175879396984927,
          0.3542713567839196,
          0.35678391959798994,
          0.3592964824120603,
          0.36180904522613067,
          0.36432160804020103,
          0.36683417085427134,
          0.3693467336683417,
          0.37185929648241206,
          0.3743718592964824,
          0.3768844221105528,
          0.37939698492462315,
          0.38190954773869346,
          0.3844221105527638,
          0.3869346733668342,
          0.38944723618090454,
          0.3919597989949749,
          0.3944723618090452,
          0.3969849246231156,
          0.39949748743718594,
          0.4020100502512563,
          0.40452261306532666,
          0.40703517587939697,
          0.40954773869346733,
          0.4120603015075377,
          0.41457286432160806,
          0.4170854271356784,
          0.41959798994974873,
          0.4221105527638191,
          0.42462311557788945,
          0.4271356783919598,
          0.4296482412060302,
          0.4321608040201005,
          0.43467336683417085,
          0.4371859296482412,
          0.4396984924623116,
          0.44221105527638194,
          0.44472361809045224,
          0.4472361809045226,
          0.44974874371859297,
          0.45226130653266333,
          0.4547738693467337,
          0.457286432160804,
          0.45979899497487436,
          0.4623115577889447,
          0.4648241206030151,
          0.46733668341708545,
          0.46984924623115576,
          0.4723618090452261,
          0.4748743718592965,
          0.47738693467336685,
          0.4798994974874372,
          0.4824120603015075,
          0.4849246231155779,
          0.48743718592964824,
          0.4899497487437186,
          0.49246231155778897,
          0.49497487437185933,
          0.49748743718592964,
          0.5,
          0.5025125628140703,
          0.5050251256281407,
          0.507537688442211,
          0.5100502512562815,
          0.5125628140703518,
          0.5150753768844221,
          0.5175879396984925,
          0.5201005025125628,
          0.5226130653266332,
          0.5251256281407035,
          0.5276381909547738,
          0.5301507537688442,
          0.5326633165829145,
          0.535175879396985,
          0.5376884422110553,
          0.5402010050251257,
          0.542713567839196,
          0.5452261306532663,
          0.5477386934673367,
          0.550251256281407,
          0.5527638190954774,
          0.5552763819095478,
          0.5577889447236181,
          0.5603015075376885,
          0.5628140703517588,
          0.5653266331658292,
          0.5678391959798995,
          0.5703517587939698,
          0.5728643216080402,
          0.5753768844221105,
          0.577889447236181,
          0.5804020100502513,
          0.5829145728643216,
          0.585427135678392,
          0.5879396984924623,
          0.5904522613065327,
          0.592964824120603,
          0.5954773869346733,
          0.5979899497487438,
          0.6005025125628141,
          0.6030150753768845,
          0.6055276381909548,
          0.6080402010050251,
          0.6105527638190955,
          0.6130653266331658,
          0.6155778894472362,
          0.6180904522613065,
          0.6206030150753769,
          0.6231155778894473,
          0.6256281407035176,
          0.628140703517588,
          0.6306532663316583,
          0.6331658291457286,
          0.635678391959799,
          0.6381909547738693,
          0.6407035175879398,
          0.6432160804020101,
          0.6457286432160804,
          0.6482412060301508,
          0.6507537688442211,
          0.6532663316582915,
          0.6557788944723618,
          0.6582914572864321,
          0.6608040201005025,
          0.6633165829145728,
          0.6658291457286433,
          0.6683417085427136,
          0.6708542713567839,
          0.6733668341708543,
          0.6758793969849246,
          0.678391959798995,
          0.6809045226130653,
          0.6834170854271356,
          0.6859296482412061,
          0.6884422110552764,
          0.6909547738693468,
          0.6934673366834171,
          0.6959798994974874,
          0.6984924623115578,
          0.7010050251256281,
          0.7035175879396985,
          0.7060301507537688,
          0.7085427135678392,
          0.7110552763819096,
          0.7135678391959799,
          0.7160804020100503,
          0.7185929648241206,
          0.7211055276381909,
          0.7236180904522613,
          0.7261306532663316,
          0.7286432160804021,
          0.7311557788944724,
          0.7336683417085427,
          0.7361809045226131,
          0.7386934673366834,
          0.7412060301507538,
          0.7437185929648241,
          0.7462311557788944,
          0.7487437185929648,
          0.7512562814070352,
          0.7537688442211056,
          0.7562814070351759,
          0.7587939698492463,
          0.7613065326633166,
          0.7638190954773869,
          0.7663316582914573,
          0.7688442211055276,
          0.771356783919598,
          0.7738693467336684,
          0.7763819095477387,
          0.7788944723618091,
          0.7814070351758794,
          0.7839195979899498,
          0.7864321608040201,
          0.7889447236180904,
          0.7914572864321608,
          0.7939698492462312,
          0.7964824120603016,
          0.7989949748743719,
          0.8015075376884422,
          0.8040201005025126,
          0.8065326633165829,
          0.8090452261306533,
          0.8115577889447236,
          0.8140703517587939,
          0.8165829145728644,
          0.8190954773869347,
          0.8216080402010051,
          0.8241206030150754,
          0.8266331658291457,
          0.8291457286432161,
          0.8316582914572864,
          0.8341708542713568,
          0.8366834170854272,
          0.8391959798994975,
          0.8417085427135679,
          0.8442211055276382,
          0.8467336683417086,
          0.8492462311557789,
          0.8517587939698492,
          0.8542713567839196,
          0.8567839195979899,
          0.8592964824120604,
          0.8618090452261307,
          0.864321608040201,
          0.8668341708542714,
          0.8693467336683417,
          0.8718592964824121,
          0.8743718592964824,
          0.8768844221105527,
          0.8793969849246231,
          0.8819095477386935,
          0.8844221105527639,
          0.8869346733668342,
          0.8894472361809045,
          0.8919597989949749,
          0.8944723618090452,
          0.8969849246231156,
          0.8994974874371859,
          0.9020100502512562,
          0.9045226130653267,
          0.907035175879397,
          0.9095477386934674,
          0.9120603015075377,
          0.914572864321608,
          0.9170854271356784,
          0.9195979899497487,
          0.9221105527638191,
          0.9246231155778895,
          0.9271356783919598,
          0.9296482412060302,
          0.9321608040201005,
          0.9346733668341709,
          0.9371859296482412,
          0.9396984924623115,
          0.9422110552763819,
          0.9447236180904522,
          0.9472361809045227,
          0.949748743718593,
          0.9522613065326633,
          0.9547738693467337,
          0.957286432160804,
          0.9597989949748744,
          0.9623115577889447,
          0.964824120603015,
          0.9673366834170855,
          0.9698492462311558,
          0.9723618090452262,
          0.9748743718592965,
          0.9773869346733669,
          0.9798994974874372,
          0.9824120603015075,
          0.9849246231155779,
          0.9874371859296482,
          0.9899497487437187,
          0.992462311557789,
          0.9949748743718593,
          0.9974874371859297,
          1
         ],
         "type": "scatter",
         "uid": "2f9d4237-f731-11e8-89c0-309c2316f643",
         "x": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "y": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        },
        {
         "line": {
          "dash": "dash"
         },
         "mode": "lines",
         "name": "baseline classifier",
         "type": "scatter",
         "uid": "2f9d4238-f731-11e8-b7cb-309c2316f643",
         "x": [
          0,
          1
         ],
         "y": [
          0,
          1
         ]
        }
       ],
       "layout": {
        "title": "Curva ROC del classificatore k-NN sul dataset Breast Cancer",
        "xaxis": {
         "title": "FPR"
        },
        "yaxis": {
         "title": "TPR"
        }
       }
      },
      "text/html": [
       "<div id=\"489b6c6d-68d7-4889-aea1-567430894508\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"489b6c6d-68d7-4889-aea1-567430894508\", [{\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=5\", \"text\": [0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0], \"x\": [1.0, 0.37037037037037035, 0.08333333333333333, 0.018518518518518517, 0.018518518518518517, 0.0], \"y\": [1.0, 0.9682539682539683, 0.9047619047619048, 0.6984126984126984, 0.6984126984126984, 0.5714285714285714], \"type\": \"scatter\", \"uid\": \"2f9d2ed0-f731-11e8-90fd-309c2316f643\"}, {\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=9\", \"text\": [0.0, 0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777777, 0.8888888888888888, 1.0], \"x\": [1.0, 0.5, 0.23148148148148148, 0.1111111111111111, 0.09259259259259259, 0.046296296296296294, 0.027777777777777776, 0.018518518518518517, 0.018518518518518517, 0.0], \"y\": [1.0, 0.9841269841269841, 0.9523809523809523, 0.9047619047619048, 0.8571428571428571, 0.8095238095238095, 0.746031746031746, 0.7142857142857143, 0.6031746031746031, 0.5396825396825397], \"type\": \"scatter\", \"uid\": \"2f9d4236-f731-11e8-b8d1-309c2316f643\"}, {\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=398\", \"text\": [0.0, 0.002512562814070352, 0.005025125628140704, 0.007537688442211055, 0.010050251256281407, 0.01256281407035176, 0.01507537688442211, 0.017587939698492462, 0.020100502512562814, 0.022613065326633167, 0.02512562814070352, 0.02763819095477387, 0.03015075376884422, 0.032663316582914576, 0.035175879396984924, 0.03768844221105528, 0.04020100502512563, 0.04271356783919598, 0.04522613065326633, 0.04773869346733668, 0.05025125628140704, 0.052763819095477386, 0.05527638190954774, 0.05778894472361809, 0.06030150753768844, 0.06281407035175879, 0.06532663316582915, 0.0678391959798995, 0.07035175879396985, 0.0728643216080402, 0.07537688442211056, 0.07788944723618091, 0.08040201005025126, 0.0829145728643216, 0.08542713567839195, 0.08793969849246232, 0.09045226130653267, 0.09296482412060302, 0.09547738693467336, 0.09798994974874373, 0.10050251256281408, 0.10301507537688442, 0.10552763819095477, 0.10804020100502512, 0.11055276381909548, 0.11306532663316583, 0.11557788944723618, 0.11809045226130653, 0.12060301507537688, 0.12311557788944724, 0.12562814070351758, 0.12814070351758794, 0.1306532663316583, 0.13316582914572864, 0.135678391959799, 0.13819095477386936, 0.1407035175879397, 0.14321608040201006, 0.1457286432160804, 0.14824120603015076, 0.15075376884422112, 0.15326633165829145, 0.15577889447236182, 0.15829145728643215, 0.16080402010050251, 0.16331658291457288, 0.1658291457286432, 0.16834170854271358, 0.1708542713567839, 0.17336683417085427, 0.17587939698492464, 0.17839195979899497, 0.18090452261306533, 0.18341708542713567, 0.18592964824120603, 0.1884422110552764, 0.19095477386934673, 0.1934673366834171, 0.19597989949748745, 0.1984924623115578, 0.20100502512562815, 0.20351758793969849, 0.20603015075376885, 0.2085427135678392, 0.21105527638190955, 0.2135678391959799, 0.21608040201005024, 0.2185929648241206, 0.22110552763819097, 0.2236180904522613, 0.22613065326633167, 0.228643216080402, 0.23115577889447236, 0.23366834170854273, 0.23618090452261306, 0.23869346733668342, 0.24120603015075376, 0.24371859296482412, 0.24623115577889448, 0.24874371859296482, 0.25125628140703515, 0.2537688442211055, 0.2562814070351759, 0.25879396984924624, 0.2613065326633166, 0.2638190954773869, 0.2663316582914573, 0.26884422110552764, 0.271356783919598, 0.27386934673366836, 0.2763819095477387, 0.27889447236180903, 0.2814070351758794, 0.28391959798994976, 0.2864321608040201, 0.2889447236180905, 0.2914572864321608, 0.29396984924623115, 0.2964824120603015, 0.2989949748743719, 0.30150753768844224, 0.30402010050251255, 0.3065326633165829, 0.30904522613065327, 0.31155778894472363, 0.314070351758794, 0.3165829145728643, 0.31909547738693467, 0.32160804020100503, 0.3241206030150754, 0.32663316582914576, 0.32914572864321606, 0.3316582914572864, 0.3341708542713568, 0.33668341708542715, 0.3391959798994975, 0.3417085427135678, 0.3442211055276382, 0.34673366834170855, 0.3492462311557789, 0.35175879396984927, 0.3542713567839196, 0.35678391959798994, 0.3592964824120603, 0.36180904522613067, 0.36432160804020103, 0.36683417085427134, 0.3693467336683417, 0.37185929648241206, 0.3743718592964824, 0.3768844221105528, 0.37939698492462315, 0.38190954773869346, 0.3844221105527638, 0.3869346733668342, 0.38944723618090454, 0.3919597989949749, 0.3944723618090452, 0.3969849246231156, 0.39949748743718594, 0.4020100502512563, 0.40452261306532666, 0.40703517587939697, 0.40954773869346733, 0.4120603015075377, 0.41457286432160806, 0.4170854271356784, 0.41959798994974873, 0.4221105527638191, 0.42462311557788945, 0.4271356783919598, 0.4296482412060302, 0.4321608040201005, 0.43467336683417085, 0.4371859296482412, 0.4396984924623116, 0.44221105527638194, 0.44472361809045224, 0.4472361809045226, 0.44974874371859297, 0.45226130653266333, 0.4547738693467337, 0.457286432160804, 0.45979899497487436, 0.4623115577889447, 0.4648241206030151, 0.46733668341708545, 0.46984924623115576, 0.4723618090452261, 0.4748743718592965, 0.47738693467336685, 0.4798994974874372, 0.4824120603015075, 0.4849246231155779, 0.48743718592964824, 0.4899497487437186, 0.49246231155778897, 0.49497487437185933, 0.49748743718592964, 0.5, 0.5025125628140703, 0.5050251256281407, 0.507537688442211, 0.5100502512562815, 0.5125628140703518, 0.5150753768844221, 0.5175879396984925, 0.5201005025125628, 0.5226130653266332, 0.5251256281407035, 0.5276381909547738, 0.5301507537688442, 0.5326633165829145, 0.535175879396985, 0.5376884422110553, 0.5402010050251257, 0.542713567839196, 0.5452261306532663, 0.5477386934673367, 0.550251256281407, 0.5527638190954774, 0.5552763819095478, 0.5577889447236181, 0.5603015075376885, 0.5628140703517588, 0.5653266331658292, 0.5678391959798995, 0.5703517587939698, 0.5728643216080402, 0.5753768844221105, 0.577889447236181, 0.5804020100502513, 0.5829145728643216, 0.585427135678392, 0.5879396984924623, 0.5904522613065327, 0.592964824120603, 0.5954773869346733, 0.5979899497487438, 0.6005025125628141, 0.6030150753768845, 0.6055276381909548, 0.6080402010050251, 0.6105527638190955, 0.6130653266331658, 0.6155778894472362, 0.6180904522613065, 0.6206030150753769, 0.6231155778894473, 0.6256281407035176, 0.628140703517588, 0.6306532663316583, 0.6331658291457286, 0.635678391959799, 0.6381909547738693, 0.6407035175879398, 0.6432160804020101, 0.6457286432160804, 0.6482412060301508, 0.6507537688442211, 0.6532663316582915, 0.6557788944723618, 0.6582914572864321, 0.6608040201005025, 0.6633165829145728, 0.6658291457286433, 0.6683417085427136, 0.6708542713567839, 0.6733668341708543, 0.6758793969849246, 0.678391959798995, 0.6809045226130653, 0.6834170854271356, 0.6859296482412061, 0.6884422110552764, 0.6909547738693468, 0.6934673366834171, 0.6959798994974874, 0.6984924623115578, 0.7010050251256281, 0.7035175879396985, 0.7060301507537688, 0.7085427135678392, 0.7110552763819096, 0.7135678391959799, 0.7160804020100503, 0.7185929648241206, 0.7211055276381909, 0.7236180904522613, 0.7261306532663316, 0.7286432160804021, 0.7311557788944724, 0.7336683417085427, 0.7361809045226131, 0.7386934673366834, 0.7412060301507538, 0.7437185929648241, 0.7462311557788944, 0.7487437185929648, 0.7512562814070352, 0.7537688442211056, 0.7562814070351759, 0.7587939698492463, 0.7613065326633166, 0.7638190954773869, 0.7663316582914573, 0.7688442211055276, 0.771356783919598, 0.7738693467336684, 0.7763819095477387, 0.7788944723618091, 0.7814070351758794, 0.7839195979899498, 0.7864321608040201, 0.7889447236180904, 0.7914572864321608, 0.7939698492462312, 0.7964824120603016, 0.7989949748743719, 0.8015075376884422, 0.8040201005025126, 0.8065326633165829, 0.8090452261306533, 0.8115577889447236, 0.8140703517587939, 0.8165829145728644, 0.8190954773869347, 0.8216080402010051, 0.8241206030150754, 0.8266331658291457, 0.8291457286432161, 0.8316582914572864, 0.8341708542713568, 0.8366834170854272, 0.8391959798994975, 0.8417085427135679, 0.8442211055276382, 0.8467336683417086, 0.8492462311557789, 0.8517587939698492, 0.8542713567839196, 0.8567839195979899, 0.8592964824120604, 0.8618090452261307, 0.864321608040201, 0.8668341708542714, 0.8693467336683417, 0.8718592964824121, 0.8743718592964824, 0.8768844221105527, 0.8793969849246231, 0.8819095477386935, 0.8844221105527639, 0.8869346733668342, 0.8894472361809045, 0.8919597989949749, 0.8944723618090452, 0.8969849246231156, 0.8994974874371859, 0.9020100502512562, 0.9045226130653267, 0.907035175879397, 0.9095477386934674, 0.9120603015075377, 0.914572864321608, 0.9170854271356784, 0.9195979899497487, 0.9221105527638191, 0.9246231155778895, 0.9271356783919598, 0.9296482412060302, 0.9321608040201005, 0.9346733668341709, 0.9371859296482412, 0.9396984924623115, 0.9422110552763819, 0.9447236180904522, 0.9472361809045227, 0.949748743718593, 0.9522613065326633, 0.9547738693467337, 0.957286432160804, 0.9597989949748744, 0.9623115577889447, 0.964824120603015, 0.9673366834170855, 0.9698492462311558, 0.9723618090452262, 0.9748743718592965, 0.9773869346733669, 0.9798994974874372, 0.9824120603015075, 0.9849246231155779, 0.9874371859296482, 0.9899497487437187, 0.992462311557789, 0.9949748743718593, 0.9974874371859297, 1.0], \"x\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"type\": \"scatter\", \"uid\": \"2f9d4237-f731-11e8-89c0-309c2316f643\"}, {\"line\": {\"dash\": \"dash\"}, \"mode\": \"lines\", \"name\": \"baseline classifier\", \"x\": [0, 1], \"y\": [0, 1], \"type\": \"scatter\", \"uid\": \"2f9d4238-f731-11e8-b7cb-309c2316f643\"}], {\"title\": \"Curva ROC del classificatore k-NN sul dataset Breast Cancer\", \"xaxis\": {\"title\": \"FPR\"}, \"yaxis\": {\"title\": \"TPR\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"489b6c6d-68d7-4889-aea1-567430894508\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"489b6c6d-68d7-4889-aea1-567430894508\", [{\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=5\", \"text\": [0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0], \"x\": [1.0, 0.37037037037037035, 0.08333333333333333, 0.018518518518518517, 0.018518518518518517, 0.0], \"y\": [1.0, 0.9682539682539683, 0.9047619047619048, 0.6984126984126984, 0.6984126984126984, 0.5714285714285714], \"type\": \"scatter\", \"uid\": \"2f9d2ed0-f731-11e8-90fd-309c2316f643\"}, {\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=9\", \"text\": [0.0, 0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777777, 0.8888888888888888, 1.0], \"x\": [1.0, 0.5, 0.23148148148148148, 0.1111111111111111, 0.09259259259259259, 0.046296296296296294, 0.027777777777777776, 0.018518518518518517, 0.018518518518518517, 0.0], \"y\": [1.0, 0.9841269841269841, 0.9523809523809523, 0.9047619047619048, 0.8571428571428571, 0.8095238095238095, 0.746031746031746, 0.7142857142857143, 0.6031746031746031, 0.5396825396825397], \"type\": \"scatter\", \"uid\": \"2f9d4236-f731-11e8-b8d1-309c2316f643\"}, {\"mode\": \"markers+lines\", \"name\": \"k-NN classifier for k=398\", \"text\": [0.0, 0.002512562814070352, 0.005025125628140704, 0.007537688442211055, 0.010050251256281407, 0.01256281407035176, 0.01507537688442211, 0.017587939698492462, 0.020100502512562814, 0.022613065326633167, 0.02512562814070352, 0.02763819095477387, 0.03015075376884422, 0.032663316582914576, 0.035175879396984924, 0.03768844221105528, 0.04020100502512563, 0.04271356783919598, 0.04522613065326633, 0.04773869346733668, 0.05025125628140704, 0.052763819095477386, 0.05527638190954774, 0.05778894472361809, 0.06030150753768844, 0.06281407035175879, 0.06532663316582915, 0.0678391959798995, 0.07035175879396985, 0.0728643216080402, 0.07537688442211056, 0.07788944723618091, 0.08040201005025126, 0.0829145728643216, 0.08542713567839195, 0.08793969849246232, 0.09045226130653267, 0.09296482412060302, 0.09547738693467336, 0.09798994974874373, 0.10050251256281408, 0.10301507537688442, 0.10552763819095477, 0.10804020100502512, 0.11055276381909548, 0.11306532663316583, 0.11557788944723618, 0.11809045226130653, 0.12060301507537688, 0.12311557788944724, 0.12562814070351758, 0.12814070351758794, 0.1306532663316583, 0.13316582914572864, 0.135678391959799, 0.13819095477386936, 0.1407035175879397, 0.14321608040201006, 0.1457286432160804, 0.14824120603015076, 0.15075376884422112, 0.15326633165829145, 0.15577889447236182, 0.15829145728643215, 0.16080402010050251, 0.16331658291457288, 0.1658291457286432, 0.16834170854271358, 0.1708542713567839, 0.17336683417085427, 0.17587939698492464, 0.17839195979899497, 0.18090452261306533, 0.18341708542713567, 0.18592964824120603, 0.1884422110552764, 0.19095477386934673, 0.1934673366834171, 0.19597989949748745, 0.1984924623115578, 0.20100502512562815, 0.20351758793969849, 0.20603015075376885, 0.2085427135678392, 0.21105527638190955, 0.2135678391959799, 0.21608040201005024, 0.2185929648241206, 0.22110552763819097, 0.2236180904522613, 0.22613065326633167, 0.228643216080402, 0.23115577889447236, 0.23366834170854273, 0.23618090452261306, 0.23869346733668342, 0.24120603015075376, 0.24371859296482412, 0.24623115577889448, 0.24874371859296482, 0.25125628140703515, 0.2537688442211055, 0.2562814070351759, 0.25879396984924624, 0.2613065326633166, 0.2638190954773869, 0.2663316582914573, 0.26884422110552764, 0.271356783919598, 0.27386934673366836, 0.2763819095477387, 0.27889447236180903, 0.2814070351758794, 0.28391959798994976, 0.2864321608040201, 0.2889447236180905, 0.2914572864321608, 0.29396984924623115, 0.2964824120603015, 0.2989949748743719, 0.30150753768844224, 0.30402010050251255, 0.3065326633165829, 0.30904522613065327, 0.31155778894472363, 0.314070351758794, 0.3165829145728643, 0.31909547738693467, 0.32160804020100503, 0.3241206030150754, 0.32663316582914576, 0.32914572864321606, 0.3316582914572864, 0.3341708542713568, 0.33668341708542715, 0.3391959798994975, 0.3417085427135678, 0.3442211055276382, 0.34673366834170855, 0.3492462311557789, 0.35175879396984927, 0.3542713567839196, 0.35678391959798994, 0.3592964824120603, 0.36180904522613067, 0.36432160804020103, 0.36683417085427134, 0.3693467336683417, 0.37185929648241206, 0.3743718592964824, 0.3768844221105528, 0.37939698492462315, 0.38190954773869346, 0.3844221105527638, 0.3869346733668342, 0.38944723618090454, 0.3919597989949749, 0.3944723618090452, 0.3969849246231156, 0.39949748743718594, 0.4020100502512563, 0.40452261306532666, 0.40703517587939697, 0.40954773869346733, 0.4120603015075377, 0.41457286432160806, 0.4170854271356784, 0.41959798994974873, 0.4221105527638191, 0.42462311557788945, 0.4271356783919598, 0.4296482412060302, 0.4321608040201005, 0.43467336683417085, 0.4371859296482412, 0.4396984924623116, 0.44221105527638194, 0.44472361809045224, 0.4472361809045226, 0.44974874371859297, 0.45226130653266333, 0.4547738693467337, 0.457286432160804, 0.45979899497487436, 0.4623115577889447, 0.4648241206030151, 0.46733668341708545, 0.46984924623115576, 0.4723618090452261, 0.4748743718592965, 0.47738693467336685, 0.4798994974874372, 0.4824120603015075, 0.4849246231155779, 0.48743718592964824, 0.4899497487437186, 0.49246231155778897, 0.49497487437185933, 0.49748743718592964, 0.5, 0.5025125628140703, 0.5050251256281407, 0.507537688442211, 0.5100502512562815, 0.5125628140703518, 0.5150753768844221, 0.5175879396984925, 0.5201005025125628, 0.5226130653266332, 0.5251256281407035, 0.5276381909547738, 0.5301507537688442, 0.5326633165829145, 0.535175879396985, 0.5376884422110553, 0.5402010050251257, 0.542713567839196, 0.5452261306532663, 0.5477386934673367, 0.550251256281407, 0.5527638190954774, 0.5552763819095478, 0.5577889447236181, 0.5603015075376885, 0.5628140703517588, 0.5653266331658292, 0.5678391959798995, 0.5703517587939698, 0.5728643216080402, 0.5753768844221105, 0.577889447236181, 0.5804020100502513, 0.5829145728643216, 0.585427135678392, 0.5879396984924623, 0.5904522613065327, 0.592964824120603, 0.5954773869346733, 0.5979899497487438, 0.6005025125628141, 0.6030150753768845, 0.6055276381909548, 0.6080402010050251, 0.6105527638190955, 0.6130653266331658, 0.6155778894472362, 0.6180904522613065, 0.6206030150753769, 0.6231155778894473, 0.6256281407035176, 0.628140703517588, 0.6306532663316583, 0.6331658291457286, 0.635678391959799, 0.6381909547738693, 0.6407035175879398, 0.6432160804020101, 0.6457286432160804, 0.6482412060301508, 0.6507537688442211, 0.6532663316582915, 0.6557788944723618, 0.6582914572864321, 0.6608040201005025, 0.6633165829145728, 0.6658291457286433, 0.6683417085427136, 0.6708542713567839, 0.6733668341708543, 0.6758793969849246, 0.678391959798995, 0.6809045226130653, 0.6834170854271356, 0.6859296482412061, 0.6884422110552764, 0.6909547738693468, 0.6934673366834171, 0.6959798994974874, 0.6984924623115578, 0.7010050251256281, 0.7035175879396985, 0.7060301507537688, 0.7085427135678392, 0.7110552763819096, 0.7135678391959799, 0.7160804020100503, 0.7185929648241206, 0.7211055276381909, 0.7236180904522613, 0.7261306532663316, 0.7286432160804021, 0.7311557788944724, 0.7336683417085427, 0.7361809045226131, 0.7386934673366834, 0.7412060301507538, 0.7437185929648241, 0.7462311557788944, 0.7487437185929648, 0.7512562814070352, 0.7537688442211056, 0.7562814070351759, 0.7587939698492463, 0.7613065326633166, 0.7638190954773869, 0.7663316582914573, 0.7688442211055276, 0.771356783919598, 0.7738693467336684, 0.7763819095477387, 0.7788944723618091, 0.7814070351758794, 0.7839195979899498, 0.7864321608040201, 0.7889447236180904, 0.7914572864321608, 0.7939698492462312, 0.7964824120603016, 0.7989949748743719, 0.8015075376884422, 0.8040201005025126, 0.8065326633165829, 0.8090452261306533, 0.8115577889447236, 0.8140703517587939, 0.8165829145728644, 0.8190954773869347, 0.8216080402010051, 0.8241206030150754, 0.8266331658291457, 0.8291457286432161, 0.8316582914572864, 0.8341708542713568, 0.8366834170854272, 0.8391959798994975, 0.8417085427135679, 0.8442211055276382, 0.8467336683417086, 0.8492462311557789, 0.8517587939698492, 0.8542713567839196, 0.8567839195979899, 0.8592964824120604, 0.8618090452261307, 0.864321608040201, 0.8668341708542714, 0.8693467336683417, 0.8718592964824121, 0.8743718592964824, 0.8768844221105527, 0.8793969849246231, 0.8819095477386935, 0.8844221105527639, 0.8869346733668342, 0.8894472361809045, 0.8919597989949749, 0.8944723618090452, 0.8969849246231156, 0.8994974874371859, 0.9020100502512562, 0.9045226130653267, 0.907035175879397, 0.9095477386934674, 0.9120603015075377, 0.914572864321608, 0.9170854271356784, 0.9195979899497487, 0.9221105527638191, 0.9246231155778895, 0.9271356783919598, 0.9296482412060302, 0.9321608040201005, 0.9346733668341709, 0.9371859296482412, 0.9396984924623115, 0.9422110552763819, 0.9447236180904522, 0.9472361809045227, 0.949748743718593, 0.9522613065326633, 0.9547738693467337, 0.957286432160804, 0.9597989949748744, 0.9623115577889447, 0.964824120603015, 0.9673366834170855, 0.9698492462311558, 0.9723618090452262, 0.9748743718592965, 0.9773869346733669, 0.9798994974874372, 0.9824120603015075, 0.9849246231155779, 0.9874371859296482, 0.9899497487437187, 0.992462311557789, 0.9949748743718593, 0.9974874371859297, 1.0], \"x\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"y\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"type\": \"scatter\", \"uid\": \"2f9d4237-f731-11e8-89c0-309c2316f643\"}, {\"line\": {\"dash\": \"dash\"}, \"mode\": \"lines\", \"name\": \"baseline classifier\", \"x\": [0, 1], \"y\": [0, 1], \"type\": \"scatter\", \"uid\": \"2f9d4238-f731-11e8-b7cb-309c2316f643\"}], {\"title\": \"Curva ROC del classificatore k-NN sul dataset Breast Cancer\", \"xaxis\": {\"title\": \"FPR\"}, \"yaxis\": {\"title\": \"TPR\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution 1.3.4 (to be completed)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "kvec = [5,9,m]\n",
    "data = [] # Qui metterò i dati per disegnare la ROC. Ogni elemento della lista sarà un oggetto di classe go.Scatter  \n",
    "AUC = [] # Area under curve. Sarà una lista con 3 elementi (un elemento per ogni valore di \"k\")\n",
    "\n",
    "for k in kvec: \n",
    "    \n",
    "    thvec = np.linspace(0,1,k+1) #Valori di soglia considerati\n",
    "\n",
    "    TPRlist = [] # In questa lista inseriro' il TPR per ogni valore di soglia. Nota che la lista viene inizializzata ogni volta che cambio \"k\"\n",
    "    FPRlist = [] # In questa lista inseriro' il FPR per ogni valore di soglia.\n",
    "    \n",
    "    \n",
    "    for th in thvec:\n",
    "\n",
    "        y_hat, y_score = knn(xTrainScaled[:,0:2], yTrain, xTestScaled[:,0:2], k, th)\n",
    "        TP = np.sum((y_hat==1) & (yTest==1))\n",
    "        TN = np.sum((y_hat==-1) & (yTest==-1))\n",
    "        FP = np.sum((y_hat==1) & (yTest==-1))\n",
    "        FN = np.sum((y_hat==-1) & (yTest==1))\n",
    "\n",
    "        TPRlist.append(TP/(TP + FN))\n",
    "        FPRlist.append(FP/(FP + TN))\n",
    "    \n",
    "    \n",
    "    AUC.append(roc_auc_score(yTest,y_score))\n",
    "    data.append(go.Scatter(x = FPRlist, y = TPRlist, text = thvec, mode = 'markers+lines', name = f'k-NN classifier for k={k}'))\n",
    "    print(f\"k = {k}: AUC: {AUC[-1]}\")\n",
    "\n",
    "\n",
    "data.append(go.Scatter(x = [0, 1], y = [0, 1], mode = 'lines', name = 'baseline classifier', line = dict(dash = 'dash')))\n",
    "layout = go.Layout(xaxis = dict(title = 'FPR'), yaxis = dict(title = 'TPR'), title = 'Curva ROC del classificatore k-NN sul dataset Breast Cancer')\n",
    "\n",
    "fig = go.Figure(data, layout)\n",
    "py.iplot(fig)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 (simile all'esercizio fatto nell'esercitazione scorsa)\n",
    "Si calcoli l'indice di accuratezza (sul test set) dell'algoritmo di classificazione k-NN (per k=9 e th=0.5) considerando un numero crescente dei possibili attributi. Nello specifico, dati i possibili attributi ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'area_worst', 'smoothness_worst'], si calcoli l'indice di accuratezza per i seguenti casi:\n",
    "- attributi considerati: ['radius_mean', 'texture_mean']\n",
    "- attributi considerati: ['radius_mean', 'texture_mean', 'perimeter_mean']\n",
    "- attributi considerati: ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean']\n",
    "- ...\n",
    "- attributi considerati: ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'area_worst', 'smoothness_worst']\n",
    "\n",
    "La cella sotto carica tutti i 9 attributi di sopra negli array ``xTrain2`` e ``xTest2``  \n",
    "\n",
    "E' vero che all'aumentare del numero di attributi l'indice di accuratezza aumenta sempre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di attributi considerati = 2\n",
      "\n",
      "Numero di attributi considerati = 3\n",
      "\n",
      "Numero di attributi considerati = 4\n",
      "\n",
      "Numero di attributi considerati = 5\n",
      "\n",
      "Numero di attributi considerati = 6\n",
      "\n",
      "Numero di attributi considerati = 7\n",
      "\n",
      "Numero di attributi considerati = 8\n",
      "\n",
      "Numero di attributi considerati = 9\n",
      "\n",
      "[0.9005847953216374, 0.8947368421052632, 0.9590643274853801, 0.9415204678362573, 0.9590643274853801, 0.9532163742690059, 0.9532163742690059, 0.9649122807017544]\n"
     ]
    }
   ],
   "source": [
    "# Solution (to be completed)\n",
    "\n",
    "index = [0,1,3,4,5,6,7,23,24] # indici degli attributi considerati nei dataset xTrain e xTest\n",
    "xTrain2 = xTrainScaled[:,index]\n",
    "xTest2 = xTestScaled[:,index]\n",
    "\n",
    "k = 9\n",
    "th = 0.5\n",
    "\n",
    "accuracyList = []\n",
    "\n",
    "\n",
    "for n in range(2,10):\n",
    "    print(f\"Numero di attributi considerati = {n}\\n\")\n",
    "    y_hat,y_score = knn(xTrain2[:,0:n], yTrain, xTest2[:,0:n], k, th)\n",
    "    accuracy, TP, FP, FN, TN, TPR, FPR = evaluatekNN(yTest,y_hat)\n",
    "    accuracyList.append(accuracy)\n",
    "\n",
    "print(accuracyList) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 (opzionale, ma vi consigliamo di provare a farlo per capire bene come funziona la cross-validation): Scelta delle features tramite K-fold cross-validation\n",
    "\n",
    "Si implementi un metodo di cross-validation per scegliere la migliore combinazione delle features tra 10 possibili combinazioni. \n",
    "\n",
    "Si consideri sempre un k-nearest-neighbor con $k=5$ e $th=0.5$.\n",
    "\n",
    "Per implementare il metodo di cross-validation, dovete ripartire il dataset di training in $K$ sottoinsiemi disgiunti (ed esaustivi). Usiamo $K=4$.\n",
    "\n",
    "La cella sotto vi mostra un possibile modo di implementare la cross-validation, usando una classe disponibile in sklearn.  Nota: Potete anche fare voi la divisione manualmente, senza usare i comandi di sklearn riportati nella cella sotto. \n",
    "\n",
    "Nota: Ripasso sulla K-fold cross-validation, per K=4. Dopo aver suddiviso il dataset in 4 sottoinsiemi, usate 3 insiemi per fare il training, e il restante per misurare l'accuratezza. Ripete l'operazione 4 volte cambiando ogni volta il set di test e calcolate l'accuratezza come media aritmetica delle accuratezze. Dovete ripete questa operazione per tutte le possibile combinazioni degli attributi e scegliere la combinazione con l'accuratezza media massima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset completo\n",
      " [[0.6610614  0.02586455]\n",
      " [0.14845517 0.24117483]\n",
      " [0.83104712 0.86673072]\n",
      " [0.19837815 0.4086998 ]\n",
      " [0.69961773 0.87803902]\n",
      " [0.34631858 0.08898565]\n",
      " [0.9562887  0.66453437]\n",
      " [0.54723535 0.11223144]\n",
      " [0.64471788 0.97676839]\n",
      " [0.04544063 0.31097905]]\n",
      "\n",
      "Splits e' una lista di K=3 tuple\n",
      "\n",
      "Split:\n",
      "Indici di training:  [0 1 3 4 6 8]\n",
      "Indici di testing:  [2 5 7 9]\n",
      "Dati di training:\n",
      " [[0.6610614  0.02586455]\n",
      " [0.14845517 0.24117483]\n",
      " [0.19837815 0.4086998 ]\n",
      " [0.69961773 0.87803902]\n",
      " [0.9562887  0.66453437]\n",
      " [0.64471788 0.97676839]]\n",
      "\n",
      "Split:\n",
      "Indici di training:  [1 2 3 5 7 8 9]\n",
      "Indici di testing:  [0 4 6]\n",
      "Dati di training:\n",
      " [[0.14845517 0.24117483]\n",
      " [0.83104712 0.86673072]\n",
      " [0.19837815 0.4086998 ]\n",
      " [0.34631858 0.08898565]\n",
      " [0.54723535 0.11223144]\n",
      " [0.64471788 0.97676839]\n",
      " [0.04544063 0.31097905]]\n",
      "\n",
      "Split:\n",
      "Indici di training:  [0 2 4 5 6 7 9]\n",
      "Indici di testing:  [1 3 8]\n",
      "Dati di training:\n",
      " [[0.6610614  0.02586455]\n",
      " [0.83104712 0.86673072]\n",
      " [0.69961773 0.87803902]\n",
      " [0.34631858 0.08898565]\n",
      " [0.9562887  0.66453437]\n",
      " [0.54723535 0.11223144]\n",
      " [0.04544063 0.31097905]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esempio: divisione di un dataset in K=3 fold\n",
    "\n",
    "# Creo un semplice dataset\n",
    "x_toy = np.random.rand(10,2) # 10 osservazioni e 2 features\n",
    "y_toy = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1, -1])\n",
    "\n",
    "print(\"Dataset completo\\n\", x_toy)\n",
    "print()\n",
    "\n",
    "import sklearn.model_selection\n",
    "K = 3\n",
    "kf = sklearn.model_selection.KFold(n_splits=K, shuffle=True)\n",
    "splits = list(kf.split(x_toy))\n",
    "print(f\"Splits e' una lista di K={len(splits)} tuple\\n\")\n",
    "\n",
    "# ogni tupla e' composta da due elementi:\n",
    "# - il primo e' una lista di indici da usare per il training\n",
    "# - il secondo e' una lista di indici da usare per il testing\n",
    "for train_index, test_index in splits:\n",
    "    print(\"Split:\")\n",
    "    print(\"Indici di training: \", train_index)\n",
    "    print(\"Indici di testing: \", test_index)\n",
    "    x_toy_train = x_toy[train_index, :]\n",
    "    y_toy_train = y_toy[train_index]\n",
    "    x_toy_test  = x_toy[test_index , :]\n",
    "    y_toy_test  = y_toy[test_index]\n",
    "    print(\"Dati di training:\\n\", x_toy_train)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Implementazione della cross validation\n",
    "\n",
    "Per una determinata scelta delle feature (ad esempio, quelle agli indici `[0,1,3,4,5,6,7,23,24]`), calcola l'accuratezza del classificatore mediante 4-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.5.1 (to be completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Comparazione tra diversi sottoinsiemi di feature\n",
    "\n",
    "Scopri quali tra le combinazioni di feature proposte sotto e' preferibile, calcolando per ciascuna delle 6 combinazioni l'accuratezza tramite K-fold cross validation.\n",
    "\n",
    "Esempio di output:\n",
    "```\n",
    "Considero le features: [0]\n",
    "Accuracy per un fold: 0.8181818181818182\n",
    "Accuracy per un fold: 0.8661971830985915\n",
    "Accuracy per un fold: 0.9225352112676056\n",
    "Accuracy per un fold: 0.8591549295774648\n",
    "Accuracy media: 0.8665172855313701\n",
    "\n",
    "Considero le features: [0, 1]\n",
    "Accuracy per un fold: 0.8531468531468531\n",
    "Accuracy per un fold: 0.8591549295774648\n",
    "Accuracy per un fold: 0.9154929577464789\n",
    "Accuracy per un fold: 0.8802816901408451\n",
    "Accuracy media: 0.8770191076529105\n",
    "\n",
    "Considero le features: [0, 1, 3]\n",
    "Accuracy per un fold: 0.8671328671328671\n",
    "Accuracy per un fold: 0.8591549295774648\n",
    "Accuracy per un fold: 0.9084507042253521\n",
    "Accuracy per un fold: 0.8802816901408451\n",
    "Accuracy media: 0.8787550477691323\n",
    "\n",
    "Considero le features: [0, 1, 3, 4, 5, 6, 7, 23, 24]\n",
    "Accuracy per un fold: 0.9300699300699301\n",
    "Accuracy per un fold: 0.971830985915493\n",
    "Accuracy per un fold: 0.971830985915493\n",
    "Accuracy per un fold: 0.9507042253521126\n",
    "Accuracy media: 0.9561090318132571\n",
    "\n",
    "Considero le features: [3, 4]\n",
    "Accuracy per un fold: 0.8671328671328671\n",
    "Accuracy per un fold: 0.8873239436619719\n",
    "Accuracy per un fold: 0.9084507042253521\n",
    "Accuracy per un fold: 0.8732394366197183\n",
    "Accuracy media: 0.8840367379099774\n",
    "\n",
    "Considero le features: [7, 23, 24]\n",
    "Accuracy per un fold: 0.9230769230769231\n",
    "Accuracy per un fold: 0.971830985915493\n",
    "Accuracy per un fold: 0.9436619718309859\n",
    "Accuracy per un fold: 0.9366197183098591\n",
    "Accuracy media: 0.9437973997833152\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considero le features: [0]\n",
      "Considero le features: [0, 1]\n",
      "Considero le features: [0, 1, 3]\n",
      "Considero le features: [0, 1, 3, 4, 5, 6, 7, 23, 24]\n",
      "Considero le features: [3, 4]\n",
      "Considero le features: [7, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "# Solution 1.5.2 (to be completed)\n",
    "\n",
    "featurecombinations = [[0                  ],\n",
    "                       [0,1                ],\n",
    "                       [0,1,3              ],\n",
    "                       [0,1,3,4,5,6,7,23,24],\n",
    "                       [    3,4            ],\n",
    "                       [            7,23,24]]\n",
    "\n",
    "K = 4 # numero di fold\n",
    "kf = sklearn.model_selection.KFold(n_splits=K, shuffle=True)\n",
    "splits = list(kf.split(x))\n",
    "\n",
    "\n",
    "k = 5 # per il knn\n",
    "th = 0.5\n",
    "\n",
    "accuracyPerFeature = [] # Ogni elemento della lista sarà l'accuratezza (media) che si ottiene per una data combinazione di features\n",
    "\n",
    "for featurecombination in featurecombinations: # Ciclo su tutte le combinazioni di features\n",
    "    x_feat = x[:, featurecombination]\n",
    "    \n",
    "    print(f\"Considero le features: {featurecombination}\")\n",
    "    \n",
    "    ### Completare..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Comparazione tra tutti i possibili sottoinsiemi di feature\n",
    "\n",
    "Considera tutti i possibili sottoinsiemi delle feature agli indici `[0,1,3,4,5,6,7,23,24]`.  Calcola per ciascun sottoinsieme (ce ne sono in tutto $2^9 - 1$) l'accuratezza mediante 4-fold cross-validation, e trova quello che restituisce accuratezza massima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1.5.3 (to be completed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
